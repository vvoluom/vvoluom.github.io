I"5M<h2 id="abstract">Abstract</h2>
<p>Decision trees are models whose structure allows for tracing an explanation of how the final decision was taken. Neural networks known as ’black box’ model, do not readily and explicitly offer an explanation of how the decision was reached. However since Neural Networks are capable of learning knowledge representation it will be very useful to develop methods that interpret the model’s decisions.
In this project Activation Maximisation will be used to search for prototypical inputs that maximise the model’s response for a quantity of interest. A pair-wise prototype comparison is then carried out under different learning conditions, such as number of classes the model deals with. The study is grounded in the area of object spatial relations recognition in images and will shed light on what models are learning about objects in 2D images which should give insight into how the system can be improved.
The spatial relation problem is one where given a subject and an object the correct spatial preposition is predicted. This problem extends beyond just predicting one correct spatial preposition as there are mulitple possible relationships associated between two objects.</p>

<h1 id="contents">Contents</h1>
<ol>
  <li><a href="#intro">Intro</a></li>
  <li><a href="#Background-and-Literature-Review">Background and Literature Review</a>
    <ol>
      <li><a href="#Preamable">Preamable</a></li>
      <li><a href="#Convolutional-Neural-Networks">Convolutional Neural Networks</a>
        <ol>
          <li><a href="#Image-Components">Image Components</a></li>
          <li><a href="#Convolutional-Layer">Convolutional Layer</a></li>
          <li><a href="#ReLU-Layer">ReLU Layer</a></li>
          <li><a href="#Pooling-Layer">Pooling Layer</a></li>
          <li><a href="#Fully-Connected-Layer">Fully Connected Layer</a></li>
          <li><a href="#Final-Layer">Final Layer</a></li>
          <li><a href="#Dropout-Layer">Dropout Layer</a></li>
          <li><a href="#Single-vs-Multi-Label-Classification">Single vs Multi Label Classification</a></li>
          <li><a href="#Training-and-Terminology">Training and Terminology</a></li>
          <li><a href="#Very-Deep-Convolutional-Networks-For-Large-Scale-Image-Recognition">Very Deep Convolutional Networks For Large-Scale Image Recognition</a></li>
        </ol>
      </li>
      <li><a href="#Visual-Relationship-Detection">Visual Relationship Detection</a>
        <ol>
          <li><a href="#Recognition-Using-Visual-Phrases">Recognition Using Visual Phrases</a></li>
          <li><a href="#Visual-Relationship-Detection-with-Language-Priors">Visual Relationship Detection with Language Priors</a></li>
          <li><a href="#Detecting-Visual-Relationships-with-Deep-Relational-Networks">Detecting Visual Relationships with Deep Relational Networks</a></li>
          <li><a href="#A-Study-on-the-Detection-of-Visual-Relationships">A Study on the Detection of Visual Relationships</a></li>
        </ol>
      </li>
      <li><a href="#Datasets">Datasets</a>
        <ol>
          <li><a href="#SpatialVOC2K:-A-Multilingual-Dataset-of-Images-with-Annotations-and-Features-for-Spatial-Relations-between-Objects">SpatialVOC2K: A Multilingual Dataset of Images with Annotations and Features for Spatial Relations between Objects</a></li>
          <li><a href="#Stanford-VRD">Stanford VRD</a></li>
        </ol>
      </li>
      <li><a href="#A-Review-on-Multi-Label-Learning-Algorithms">A Review on Multi-Label Learning Algorithms</a></li>
      <li><a href="#Activation-Maximization">Activation Maximization</a></li>
    </ol>
  </li>
  <li><a href="#Methodology">Methodology</a>
    <ol>
      <li><a href="#Data-Preparation">Data Preparation</a>
        <ol>
          <li><a href="#Stanford-VRD-Dataset">Stanford VRD Dataset</a></li>
          <li><a href="#SpatialVoc2k-Dataset">SpatialVoc2k Dataset</a></li>
          <li><a href="#Geometric-Datasets">Geometric Datasets</a></li>
          <li><a href="#Single-Label-Datasets">Single Label Datasets</a></li>
        </ol>
      </li>
      <li><a href="#Image-Preparation">Image Preparation</a></li>
      <li><a href="#Training">Training</a>
        <ol>
          <li><a href="#VGG16">VGG16</a></li>
          <li><a href="#Feed-Forward-Neural-Network">Feed Forward Neural Network</a></li>
          <li><a href="#Data-Generators">Data Generators</a></li>
        </ol>
      </li>
      <li><a href="#Evaluation-and-Metrics">Evaluation and Metrics</a></li>
      <li><a href="#Interpreting-the-models">Interpreting the models</a></li>
    </ol>
  </li>
  <li><a href="#Findings">Findings</a>
    <ol>
      <li><a href="#Preamble">Preamble</a></li>
      <li><a href="#VRD-Dataset-Evaluation-Results">VRD Dataset Evaluation Results</a>
        <ol>
          <li><a href="#VGG16-Evaluation-Results">VGG16 Evaluation Results</a></li>
          <li><a href="#Feed-Forward-Evaluation-Results">Feed Forward Evaluation Results</a></li>
          <li><a href="#Activation-Maximization-Evaluation-Results">Activation Maximization Evaluation Results</a></li>
        </ol>
      </li>
      <li><a href="#SpatialVoc2k-Dataset-Evaluation-Results">SpatialVoc2k Dataset Evaluation Results</a>
        <ol>
          <li><a href="#VGG16-Evaluation-Results">VGG16 Evaluation Results</a></li>
          <li><a href="#Feed-Forward-Evaluation-Results">Feed Forward Evaluation Results</a></li>
          <li><a href="#Activation-Maximization-Evaluation-Results">Activation Maximization Evaluation Results</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#Analysis-of-Results">Analysis of Results</a>
    <ol>
      <li><a href="#Preamble">Preamble</a></li>
      <li><a href="#VRD-Results">VRD Results</a>
        <ol>
          <li><a href="#Multi-label-vs-Single-label-Classification">Multi-label vs Single-label Classification</a></li>
          <li><a href="#VGG16-vs-Feed-Forward-Neural-Network">VGG16 vs Feed Forward Neural Network</a></li>
          <li><a href="#Activation-Maximization">Activation Maximization</a></li>
          <li><a href="#Conclusions">Conclusions</a></li>
        </ol>
      </li>
      <li><a href="#SpatialVoc2k-Results">SpatialVoc2k Results</a>
        <ol>
          <li><a href="#Multi-label-vs-Single-label-Classification">Multi-label vs Single-label Classification</a></li>
          <li><a href="#VGG16-vs-Feed-Forward-Neural-Network">VGG16 vs Feed Forward Neural Network</a></li>
          <li><a href="#Activation-Maximization">Activation Maximization</a></li>
          <li><a href="#Conclusions">Conclusions</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#References">References</a></li>
</ol>

<h1 id="intro">Intro</h1>
<p>       Research in computer vision has excelled in recent years largely due to technological
advancements in hardware. This allowed for more computationally intensive ideas to be
explored and implemented. Deep Learning a subset of Machine Learning is one of those
ideas based on artificial neural networks. Deep learning in computer vision comes in the
form of Convolutional Neural Networks (CNN’s). This form of Machine Learning is effective as it takes the given images and through a sequence of convolutions and pooling layers
transforms this data into that of smaller size while retaining important features. This
reduces the training time and computational power required to classify images compared
to a Neural Network.<br />
       CNNs have become very precise and effective in solving the problems of object detection and localization in images. Up until recently it was thought to be impossible for a
computer to distinguish between a cat and a dog in an image due to them having similar
general features but nowadays anyone can implement a simple CNN to solve this classification problem. The focus is shifting from object recognition to the study of visual
relationships between objects in an image. This is the visual relationship detection (VRD)
problem. In this problem given a subject and an object, the machine learning model must
predict the best predicate that describes the visual relationship between those two objects.<br />
       The VRD problem was first tackled by Sadeghi and Farhadis (2011) by taking triplet
representation &lt; subject, predicate, object &gt; as a class, this has led to an exponential
growth in classes and a long tail distribution problem. A solution to that was to divide
the problem up into parts. The first part would be to perform object detection on the two
objects and then pass their Union into a new network which was specialized in predicate
prediction as done by Lu et al (2016). There have been improvements to accuracy for
this method such as having geometric and text features accompany the CNN model for
increased accuracy. VRD is important as it would give greater context to images which
would provide real world solutions to problems such as giving audio descriptions to blind
people of the environment around them.<br />
       Since neural networks are a black box models to know if a CNN is working correctly
it is evaluated over unseen data and its accuracy is measured over how well it predicts
this data. This is a working method however we don’t understand how and why the final
descion was made. It would be reassuring to understand why the decision was made. An
example of why this is important, in the news there was a lady who fell asleep at the wheel
of a self driving car and this car didnt stop when a pedestrian was crossing the road and hit them. When the company looked at the logs of the car to figure out what went wrong
and why the car didnt see the pedestrian they found out that the car had seen the person
and decided not to stop. If all the descion making process of the car had been carefully
understood and analyzed the people creating the A.I could have seen that in one of those
decision paths the car would see the person and not stop. As A.I systems are being integrated into our daily lives such as medical diagnosis and driverless cars, it is important to
make sure they are understood and Activation Maximization is one of the methods that
will be explored in this dissertation.<br />
       The research aim is to use Activation Maximization on the VRD problem to interpret
and understand what the CNN is looking for when classifiying relations. Since VRD contains many relationships the main focus will be on spatial relations between two objects.
This dissertation will focus on interpreting different models and configurations to have an
understanding of what the model is learning and whether or not Activation Maximization
is a useful method of doing so.</p>

<h1 id="background-and-literature-review">Background and Literature Review</h1>

<h2 id="preamable">Preamable</h2>

<p>In this chapter we will be going through the components that make up a CNN and the
different architectures of CNNs. This chapter also includes literature reviews on existing works tackling the VRD problem and Activation Maximization.</p>

<h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2>

<h3 id="image-components">Image Components</h3>

<p>RGB images consists of 3 channels Red, Green and Blue. These 3 channels are each
represtented by a 2D array with each entry being a pixel value that ranges from 0 to 255. The pixel value in the array represents the colour intensity of the channel. Combining these 3 arrays together will yield the original image. The image details are represented as (Height ,Width,Channels) and this is the shape that the CNN expects as input. This input shape is kept the same for all images during training.</p>

<h3 id="convolutional-layer">Convolutional Layer</h3>

<p>The convolutional layer is composed of a kernel/filter which is a 2D matrix of a given size e.g (3x3) that performs matrix multiplication between itself and a portion of a region of the image. It does this by striding from left to right with a certain stride range e.g (Stride = 1 ) until the entire image is traversed. This extracts features from the image, for the first convolutional layer it would extract low level features such as colours and edges as the CNN gets deeper with more layers the features will increase in complexity to become high level features such as the wheels of car, ears of a cat, tail of a dog etc. The kernel performs two types of operations on the image, one where the feature dimensionality is
reduced compared to input and another where it is increased or stays the same due to padding. Padding is when the image width and height is increased and the pixel values that have been added are filled with 0, e.g Padding = 1 would increase the image width and height by 2. This is done as there could be valuable information in the pixel values on the outskirts of the image which would otherwise be lost as the kernel would combine them with the inner pixels.</p>

<h3 id="pooling-layer">Pooling Layer</h3>

<p>The pooling layer is used to decrease the computational power required to process data by reducing the spatial size of the convoluted features. The dominant features are extracted without losing major information and keeping the training model effective. The two types of pooling are average pooling which returns the average of all the values from the region and max pooling which returns the maximum value. Max pooling performs better than average pooling as average pooling mostly reduces the dimensionality but max pooling acts as a noise suppressant by discarding low values (Noise).</p>

<h3 id="fully-connected-layer">Fully Connected Layer</h3>

<p>After a multitude of Convolutional,ReLU and Pooling layers the final output is passed through a fully connected layer which is where the high-level reasoning and decision making is done. Here the output from the last pooling layer is Flattened meaning the image output goes from a 2D array to a 1D array by concatenating the rows from underneath each other to the right side of the rows above. Then all the values are passed into neurons in a fully connected layer which all have connections to the activations as done in a regular Neural Network.</p>

<h3 id="final-layer">Final Layer</h3>

<p>The last layer is the output layer where all the neurons from the fully connected layer connect to. They connect to a specifically set amount of neurons which is defined by the amount of classes the CNN is being trained for. This last Densly Connected layer has an activation function. The final activation function of the CNN depends on the problem being solved. In this dissertation we will be focusing on two types of problems a Single Label Classification (SLC) problem and a Multi Label Classification (MLC) problem.</p>

<h3 id="dropout-layer">Dropout Layer</h3>

<p>This layer is used to reduce overfitting of the training data on the model being trained. Overfitting is when the model doesnt generalize the parameters enough and represents the training data too much, meaning it would keep getting increasingly better accuracy during training but would have decreasing accuracy on the validation data. The dropout layer combats this by disabling neurons by setting them to 0 at each training stage which have a probability less than p = 0.5. This increases generalization as it forces the layer to learn the same concept with new neurons.</p>

<h3 id="single-vs-multi-label-classification">Single vs Multi Label Classification</h3>

<p>The SLC problem consists of having only one correct label associated to the input. This problem would require for the last activation function to be a Softmax function. Softmax outputs a range of probabilities per class that all add up to 100%, the highest probability is considered as the best label. The MLC problem consists of having multiple correct labels associated to the input. This
problem would require for the last activation function to be a Sigmoid function. Sigmoid outputs a range of probabilities per class where each class ranges from 0% to 100% individually and independant of the other classes. Unlike the Softmax output the probabilities do not necessarily add up to 100% therefore a threshold is usually set to what is an accepted output or not e.g anything above 50% probability is accepted.</p>

<h3 id="training-and-terminology">Training and Terminology</h3>

<p>Once the structure of the CNN is setup it needs parameters for it to be compiled. The parameters consist of a loss function,optimizer and a metric. The loss function dictates how the model is penalized when the predicted values deviate from the true values. The optimizers job is to make sure that the loss function is minimized as much as possible. Finally the metric used is what shows the final accuracy of the current training cycle between the predicted and the true values. The dataset would be seperated into 3 parts training/testing/validation , the training data is used to train the CNN and the test data is used to evaluate the trained model on unseen data. The validation data is used during training as unseen data to show if the model is being overfitted during training, it can be
seen that it is overfitting when the training accuracy is going up and loss is going down but the opposite is occurring for the validation accuracy and loss.</p>

<h3 id="very-deep-convolutional-networks-for-large-scale-image-recognition">Very Deep Convolutional Networks For Large-Scale Image Recognition</h3>

<p>In attempt to improve CNNs at the time, mainly the original architecture of Krizhevsky et al. (2012). The Visual Geometry Group focused on the depth of the convolutional network an important aspect which affects how high the level of the features could be learned. The architecture had its parameters fixed and the convolutional layers increased thus increasing the depth of the network, this was feasible as small kernels (3x3) had been utilized in all the layers. This method produced better results than previous architectures at the time. The two best performing models weights have been released to the public. The CNN was trained using fixed-size 224x224 RGB images which had been preprocessed by subtracting the mean RGB value from each pixel. The images had been then passed through a stack of convolutional and max pooling layers with a window of 2x2 pixels of stride 2. The CNN has 5 max pooling layers and not all the convolutional layers are followed by a max pooling layer. The convolutional strides are performed by a kernel of
size (3x3) with Stride = 1 and a padding of ’Same’ to preserve the spatial resolution after the convolution. Each hidden layer is equipped with a ReLU function, the architecture of VGG16 is dhown in Figure 1. After all the convolutional and max pooling layers there are 3 fully connected layers with the first two having 4096 neurons each and the last densely connected layer having 1000 output neurons for every class. The models had been trained as a SLC problem therefore the last activation function is Softmax. The top performing models had 16 and 19 layers therefore their acquired names are Visual Geometry Group
16 (VGG16) and Visual Geometry Group 19 (VGG19).<br />
       The model was trained using mini-batch processing with a batch size of 256. The loss function used was stochastic gradient descent(SGD) with a momentum of 0.9, drop out ratio of 0.5 and was trained for 74 epochs. The initial learning rate was 0.1 and was decreased by a factor of 10 when accuracy stagnated. It was trained using four NVIDIA
Titan Black GPUs for two to three weeks.</p>

<p><img src="../images/ActivationMaximization/Vgg16Arch.png" alt="linearly separable data" /></p>
:ET